## Prometheus告警处理
From: https://prometheus.io/docs/alerting/configuration/

本节我们将带来读者探索Prometheus的告警处理机制，在前面的部分已经介绍了在Prometheus在Prometheus的架构中被划分为两个部分，在Promtheus Server中定义告警规则以及产生告警，Alertmanager组件则用于处理这些由Promtheus产生的告警。Alertmanager提供了多种内置第三方告警通知方式，同时还提供了对Webhook通知的支持，通过Webhook用户可以完成对告警更多个性化的扩展。

本节主要内容：

* 在Promtheus中自定义告警规则
* 理解Alertmanager特性
* 基于标签的动态告警处理
* 将告警通知发送到第三方服务
* 如何使用Webhook扩展Alertmanager
* 以及一些其他的性能优化模式

### 理解Alertmanager

Alertmanager作为一个独立的组件，负责接收并处理来自Prometheus Server(也可以是其它的客户端程序)的告警信息。

Alertmanager可以对这些告警信息进行进一步的处理，比如消除重复的告警信息，对告警信息进行分组并且路由到正确的接受方，Prometheus内置了对邮件，Slack等通知方式的支持，同时还支持与Webhook的通知集成，以支持更多的可能性，例如可以通过Webhook与钉钉或者企业微信进行集成。同时AlertManager还提供了静默和告警抑制机制来对告警通知行为进行优化。

### Alertmanager特性
#### 分组
分组机制可以将详细的告警信息合并成一个通知。在某些情况下，比如由于系统宕机导致大量的告警被同时触发，在这种情况下分组机制可以将这些被触发的告警合并为一个告警通知，避免一次性接受大量的告警通知，而无法对问题进行快速定位。

例如，当集群中有数百个正在运行的服务实例，并且为每一个实例设置了告警规则。假如此时发生了网络故障，可能导致大量的服务实例无法连接到数据库，结果就会有数百个告警被发送到Alertmanager。

而作为用户，可能只希望能够在一个通知中中就能查看哪些服务实例收到影响。这时可以按照服务所在集群或者告警名称对告警进行分组，而将这些告警内聚在一起成为一个通知。

告警分组，告警时间，以及告警的接受方式可以通过Alertmanager的配置文件进行配置。

#### 抑制
抑制是指当某一告警发出后，可以停止重复发送由此告警引发的其它告警的机制。

例如，当集群不可访问时触发了一次告警，通过配置Alertmanager可以忽略与该集群有关的其它所有告警。这样可以避免接收到大量与实际问题无关的告警通知。

抑制机制同样通过Alertmanager的配置文件进行设置。

#### 静默
静默提供了一个简单的机制可以快速根据标签对告警进行静默处理。如果接收到的告警符合静默的配置，Alertmanager则不会发送告警通知。

静默设置需要在Alertmanager的Werb页面上进行设置。

### 定义告警规则
告警规则文件使用Yaml格式进行定义，一个典型的告警配置如下：
```
groups:
- name: example
  rules:
  - alert: HighErrorRate
    expr: job:request_latency_seconds:mean5m{job="myjob"} > 0.5
    for: 10m
    labels:
      severity: page
    annotations:
      summary: High request latency
      description: description info
```
在告警规则文件中，我们可以将一组相关的规则设置定义在一个group下。在每一个group中我们可以定义多个告警规则(rule)。一条告警规则主要由以下几部分组成：

* alert：告警规则的名称。
* expr：基于PromQL表达式告警触发条件，用于计算是否有时间序列满足该条件。
* for：评估等待时间，可选参数。用于表示只有当触发条件持续一段时间后才发送告警。在等待期间新产生告警的状态为pending。
* labels：自定义标签，允许用户指定要附加到告警上的一组附加标签。
* annotations：用于指定一组附加信息，比如用于描述告警详细信息的文字等。

Prometheus根据global.evaluation_interval定义的周期计算PromQL表达式。如果PromQL表达式能够找到匹配的时间序列则会为每一条时间序列产生一个告警实例。

### 模板化
一般来说，在告警规则文件的annotations中使用summary描述告警的概要信息，description用于描述告警的详细信息。同时Alertmanager的UI也会根据这两个标签值，显示告警信息。为了让告警信息具有更好的可读性，Prometheus支持模板化label和annotations的中标签的值。

通过$labels.变量可以访问当前告警实例中指定标签的值。$value则可以获取当前PromQL表达式计算的样本值。
```
# To insert a firing element's label values:
{{ $labels.<labelname> }}
# To insert the numeric expression value of the firing element:
{{ $value }}
```
例如，可以通过模板化优化summary以及description的内容的可读性：
```
groups:
- name: example
  rules:

  # Alert for any instance that is unreachable for >5 minutes.
  - alert: InstanceDown
    expr: up == 0
    for: 5m
    labels:
      severity: page
    annotations:
      summary: "Instance {{ $labels.instance }} down"
      description: "{{ $labels.instance }} of job {{ $labels.job }} has been down for more than 5 minutes."

  # Alert for any instance that has a median request latency >1s.
  - alert: APIHighRequestLatency
    expr: api_http_request_latencies_second{quantile="0.5"} > 1
    for: 10m
    annotations:
      summary: "High request latency on {{ $labels.instance }}"
      description: "{{ $labels.instance }} has a median request latency above 1s (current value: {{ $value }}s)"
```

### 查看告警状态
用户可以通过Prometheus WEB界面中的Alerts菜单查看当前Prometheus下的所有告警规则，以及其当前所处的活动状态。

同时对于已经pending或者firing的告警，Prometheus也会将它们存储到时间序列ALERTS{}中。

可以通过表达式，查询告警实例：
```
ALERTS{alertname="<alert name>", alertstate="pending|firing", <additional alert labels>}
```
样本值为1表示当前告警处于活动状态（pending或者firing），当告警从活动状态转换为非活动状态时，样本值则为0。

### 实例：定义主机监控告警
修改Prometheus配置文件prometheus.yml,添加以下配置：
每隔 5m 会自动发送，直到服务恢复正常，报警解除为止，同时会发送一封报警解除邮件

rule_files:
  - /etc/prometheus/rules/*.rules
在目录/etc/prometheus/rules/下创建告警文件hoststats-alert.rules，内容如下：
```
groups:
- name: node_rules # 规则组名称：node_rules,在服务器中必须唯一
  interval: 10s    # 每10s运行一次
  rules:
  # 记录规则

  # 每台node每5分钟范围内cpu使用率百分比
  - record: instance:node_cpu:avg_rate
    expr: 100 - avg(irate(node_cpu_seconds_total{job="node",mode="idle"}[5m])) by (instance) * 100
    labels:
      metric_type: aggregation  # 标签类型：aggragation

  # 添加报警规则
  - alert: HighNodeCPU
    expr: instance:node_cpu:avg_rate > 50
    for: 60m
    labels:
      serverity: warning
    annotations:
      summary: High Node CPU for 1 hour
      console: You might want to check the node dashBoard at localhost:9090/rules
```      
重启Prometheus服务
```
sudo service prometheus restart
```

访问Prometheus UI http://127.0.0.1:9090/rules 可以查看当前以加载的规则文件。

消耗cpu脚本： tools/killCpu.sh
```
./killCpu.sh 3
```
**参数3**表示消耗3颗CPU的资源

切换到Alerts标签http://127.0.0.1:9090/alerts可以查看当前告警的活动状态。

本地测试服务：
!["altermanager"](https://s2.ax1x.com/2020/01/16/lvw1yt.png "alertmanager")